{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"collapsed":true},"cell_type":"markdown","source":"# Sentiment Analysis and Prediction of Review Ratings on the Yelp Reviews Dataset using various Machine Learning Algorithms\nDataset Information: <br>\n(1). Dataset:\n    *   Column 1 - Unique Business ID\n    *   Column 2 - Date of Review\n    *   Column 3 - Review ID\n    *   Column 4 - Stars given by the user\n    *   Column 5 - Review given by the user\n    *   Column 6 - Type of text entered - Review\n    *   Column 7 - Unique User ID\n    *   Column 8 - Cool column: The number of cool votes the review received\n    *   Column 9 - Useful column: The number of useful votes the review received\n    *   Column 10 - Funny Column: The number of funny votes the review received <br>\n(2). Number of entries - 10000"},{"metadata":{"_uuid":"4c8f9cd6d40cc053823bcefcd76f898bfc0aed95"},"cell_type":"markdown","source":"**(1). Importing all the necessary modules:**"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true,"collapsed":true},"cell_type":"code","source":"# IMPORTING ALL THE NECESSARY LIBRARIES AND PACKAGES\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport nltk\nfrom nltk.corpus import stopwords\nimport string\nimport math\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn.grid_search import GridSearchCV\n%matplotlib inline","execution_count":7,"outputs":[]},{"metadata":{"_uuid":"38fd0e6983fa85b2f9752c16ceb680b6ad40365e"},"cell_type":"markdown","source":"**(2). Loading and seeing the dataset details:**"},{"metadata":{"trusted":true,"_uuid":"1eaa689d053a6fb89bbb697663821fa6a9788aa2"},"cell_type":"code","source":"# LOADING THE DATASET AND SEEING THE DETAILS\ndata = pd.read_csv('../input/yelp.csv')\n# SHAPE OF THE DATASET\nprint(\"Shape of the dataset:\")\nprint(data.shape)\n# COLUMN NAMES\nprint(\"Column names:\")\nprint(data.columns)\n# DATATYPE OF EACH COLUMN\nprint(\"Datatype of each column:\")\nprint(data.dtypes)\n# SEEING FEW OF THE ENTRIES\nprint(\"Few dataset entries:\")\nprint(data.head())\n# DATASET SUMMARY\ndata.describe(include='all')","execution_count":8,"outputs":[]},{"metadata":{"_uuid":"4cfd5ab002d32abaed2fb113ef19292878fe3358"},"cell_type":"markdown","source":"**(3). Creating of a new column:**<br>\nThe new column will be - \"length\". This column will hold the data of the word length of the review."},{"metadata":{"trusted":true,"_uuid":"c74e401bf74f0a0a2cd7f05a57e99f6f0d4fefff"},"cell_type":"code","source":"#CREATING A NEW COLUMN IN THE DATASET FOR THE NUMBER OF WORDS IN THE REVIEW\ndata['length'] = data['text'].apply(len)\ndata.head()","execution_count":9,"outputs":[]},{"metadata":{"_uuid":"f3da559ffb39c186659f2f488d538d933665d9c7"},"cell_type":"markdown","source":"**(4). Visualization:**<br>\nLet us now visualize the if there is any correlation between stars and the length of the review."},{"metadata":{"trusted":true,"_uuid":"d8d798fb8a33267eef690ebf3d6e289076300364"},"cell_type":"code","source":"# COMPARING TEXT LENGTH TO STARS\ngraph = sns.FacetGrid(data=data,col='stars')\ngraph.map(plt.hist,'length',bins=50,color='blue')","execution_count":10,"outputs":[]},{"metadata":{"_uuid":"2ef6accae543ad39527ee690f14bd67b56b2aeb6"},"cell_type":"markdown","source":"**(5). Mean Value of the Vote columns**\nThere are 3 voting columns for the reviews - funny, cool and useful. Let us now find the mean values with respect to the stars given to the review."},{"metadata":{"trusted":true,"_uuid":"0d33fbaf7bf9c99437ecc8ce8f3da33e017685d2"},"cell_type":"code","source":"# GETTING THE MEAN VALUES OF THE VOTE COLUMNS WRT THE STARS ON THE REVIEW\nstval = data.groupby('stars').mean()\nstval","execution_count":11,"outputs":[]},{"metadata":{"_uuid":"49bbd0cf96c7dd3923642d0d936a9d8c32728029"},"cell_type":"markdown","source":"**(6). Correlation between the voting columns:** <br>\nLet us now see what the correlation is between the three voting columns."},{"metadata":{"trusted":true,"_uuid":"2b4a85469a460276be7c113d917515979b27ceae"},"cell_type":"code","source":"# FINDING THE CORRELATION BETWEEN THE VOTE COLUMNS\nstval.corr()","execution_count":12,"outputs":[]},{"metadata":{"_uuid":"5e25c839d200a3afc82f3eff86ace27a9ba700b6"},"cell_type":"markdown","source":"Thus, we can see that there is negative correlation between:\n    * Cool and Useful\n    * Cool and Funny\n    * Cool and Length  <br>\nThus, we can say that the reviews marked cool tend to be curt, not very useful to others and short.<br>\nWhereas, there is a positive correlation between:\n    * Funny and Useful    \n    * Funny and Length\n    * Useful and Length    \nThus, we can say that longer reviews tend to be funny and useful."},{"metadata":{"_uuid":"2aaab13300bade1599709fc829b6ae4c455cd508"},"cell_type":"markdown","source":"**(7). Classifying the dataset and splitting it into the reviews and stars:**"},{"metadata":{"trusted":true,"_uuid":"6bdff07af1a991b91e699eb1dddeedaec1894a38"},"cell_type":"code","source":"# CLASSIFICATION\ndata_classes = data[(data['stars']==1) | (data['stars']==3) | (data['stars']==5)]\ndata_classes.head()\nprint(data_classes.shape)\n\n# Seperate the dataset into X and Y for prediction\nx = data_classes['text']\ny = data_classes['stars']\nprint(x.head())\nprint(y.head())","execution_count":13,"outputs":[]},{"metadata":{"_uuid":"5908ff3871140a101cd172a68bf10bf936dad8ea"},"cell_type":"markdown","source":"**(8). Data Cleaning:** <br>\nWe will now, define a function which will clean the dataset by removing stopwords and punctuations."},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"0412473cc8b5458282c0bed7b6702763e9e0d4bc"},"cell_type":"code","source":"# CLEANING THE REVIEWS - REMOVAL OF STOPWORDS AND PUNCTUATION\ndef text_process(text):\n    nopunc = [char for char in text if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]","execution_count":86,"outputs":[]},{"metadata":{"_uuid":"91a0a916ad6a1a3e2efb898acc0d553e24229003"},"cell_type":"markdown","source":"**(9). Vectorization**<br>\nWe will now vectorize a single review and see the results:"},{"metadata":{"trusted":true,"_uuid":"b4d9c7dd415b17a37428803a64f9a629ea546fd3","collapsed":true},"cell_type":"code","source":"# CONVERTING THE WORDS INTO A VECTOR\nvocab = CountVectorizer(analyzer=text_process).fit(x)\nprint(len(vocab.vocabulary_))\nr0 = x[0]\nprint(r0)\nvocab0 = vocab.transform([r0])\nprint(vocab0)\n\"\"\"\n    Now the words in the review number 78 have been converted into a vector.\n    The data that we can see is the transformed words.\n    If we now get the feature's name - we can get the word back!\n\"\"\"\nprint(\"Getting the words back:\")\nprint(vocab.get_feature_names()[19648])\nprint(vocab.get_feature_names()[10643])","execution_count":87,"outputs":[]},{"metadata":{"_uuid":"5e46409ff84d460ab507b6f8f5570045e0545c99"},"cell_type":"markdown","source":"**(10). Vectorization of the whole review set and and checking the sparse matrix:**"},{"metadata":{"trusted":true,"_uuid":"fd0230355f93564ff0d66179f047827b594a1f36","collapsed":true},"cell_type":"code","source":"x = vocab.transform(x)\n#Shape of the matrix:\nprint(\"Shape of the sparse matrix: \", x.shape)\n#Non-zero occurences:\nprint(\"Non-Zero occurences: \",x.nnz)\n\n# DENSITY OF THE MATRIX\ndensity = (x.nnz/(x.shape[0]*x.shape[1]))*100\nprint(\"Density of the matrix = \",density)","execution_count":88,"outputs":[]},{"metadata":{"_uuid":"2d64b4ad5480f96361514ebbe1c2e75a58ecc369"},"cell_type":"markdown","source":"**(11). Splitting the dataset X into training and testing set:**"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"03cfa07d6a627b72c78b904eac2b6b428459b5c8"},"cell_type":"code","source":"# SPLITTING THE DATASET INTO TRAINING SET AND TESTING SET\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=101)","execution_count":89,"outputs":[]},{"metadata":{"_uuid":"c11143ea96a27b2fcf4f4b358b20b2214c853903"},"cell_type":"markdown","source":"**(12). Modelling:**<br>\nWe will now use multiple Machine Algorithms to see which gives the best performance."},{"metadata":{"_uuid":"6f7541a4cb37736208028acf340a99065cccad92"},"cell_type":"markdown","source":"(1). Multinomial Naive Bayes - We are using Multinomial Naive Bayes over Gaussian because with sparse data, Gaussian Naive Bayes assumptions aren't met and a simple gaussian fit over the data will not give us a good fit or prediction!"},{"metadata":{"trusted":true,"_uuid":"0bd46fa6e86db8c9aa27b8ff21f142cbd33c1322","collapsed":true},"cell_type":"code","source":"# Multinomial Naive Bayes\nfrom sklearn.naive_bayes import MultinomialNB\nmnb = MultinomialNB()\nmnb.fit(x_train,y_train)\npredmnb = mnb.predict(x_test)\nprint(\"Confusion Matrix for Multinomial Naive Bayes:\")\nprint(confusion_matrix(y_test,predmnb))\nprint(\"Score:\",round(accuracy_score(y_test,predmnb)*100,2))\nprint(\"Classification Report:\",classification_report(y_test,predmnb))","execution_count":90,"outputs":[]},{"metadata":{"_uuid":"dfe152bbbedee29cea92e80a0796cff143f68cfe"},"cell_type":"markdown","source":"(2). Random Forest Classifier"},{"metadata":{"trusted":true,"_uuid":"f47f04199e86368eea7bcb89dacf6ee3e36eb32b","collapsed":true},"cell_type":"code","source":"# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\nrmfr = RandomForestClassifier()\nrmfr.fit(x_train,y_train)\npredrmfr = rmfr.predict(x_test)\nprint(\"Confusion Matrix for Random Forest Classifier:\")\nprint(confusion_matrix(y_test,predrmfr))\nprint(\"Score:\",round(accuracy_score(y_test,predrmfr)*100,2))\nprint(\"Classification Report:\",classification_report(y_test,predrmfr))","execution_count":91,"outputs":[]},{"metadata":{"_uuid":"9dd89695b2dfdac56ec21f565f2e04e427a94b78"},"cell_type":"markdown","source":"(3). Decision Tree"},{"metadata":{"trusted":true,"_uuid":"1667c0c25fdb50872b68d7c03e6a9cc7c3cd959d","collapsed":true},"cell_type":"code","source":"# Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\npreddt = dt.predict(x_test)\nprint(\"Confusion Matrix for Decision Tree:\")\nprint(confusion_matrix(y_test,preddt))\nprint(\"Score:\",round(accuracy_score(y_test,preddt)*100,2))\nprint(\"Classification Report:\",classification_report(y_test,preddt))","execution_count":92,"outputs":[]},{"metadata":{"_uuid":"30b9fe08af4cbd989fe6ef3a21d156d6caf2b2af"},"cell_type":"markdown","source":"(4). Support Vector Machines"},{"metadata":{"trusted":true,"_uuid":"ef00d05e7aebf2fa0f56f5e58c5f32ef5ebd92e2","collapsed":true},"cell_type":"code","source":"# Support Vector Machine\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=101)\nsvm.fit(x_train,y_train)\npredsvm = svm.predict(x_test)\nprint(\"Confusion Matrix for Support Vector Machines:\")\nprint(confusion_matrix(y_test,predsvm))\nprint(\"Score:\",round(accuracy_score(y_test,predsvm)*100,2))\nprint(\"Classification Report:\",classification_report(y_test,predsvm))","execution_count":93,"outputs":[]},{"metadata":{"_uuid":"7d6b32501847725e3163d0fd9441d44c800b5173"},"cell_type":"markdown","source":"(5). Gradient Boosting Classifier"},{"metadata":{"trusted":true,"_uuid":"b2a4d2c63064a019729c476df7923be2daa17202","collapsed":true},"cell_type":"code","source":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\"\"\"# parameter evaluation\ngbe = GradientBoostingClassifier(random_state=0)\nparameters = {\n     'learning_rate': [0.05, 0.1, 0.5],\n    'max_features': [0.5, 1],\n    'max_depth': [3, 4, 5]}\ngridsearch=GridSearchCV(gbe,parameters,cv=100,scoring='roc_auc')\ngridsearch.fit(x,y)\nprint(gridsearch.best_params_)\nprint(gridsearch.best_score_)\"\"\"\n#Boosting\ngbi = GradientBoostingClassifier(learning_rate=0.1,max_depth=5,max_features=0.5,random_state=999999)\ngbi.fit(x_train,y_train)\npredgbi = gbi.predict(x_test)\nprint(\"Confusion Matrix for Gradient Boosting Classifier:\")\nprint(confusion_matrix(y_test,predgbi))\nprint(\"Score:\",round(accuracy_score(y_test,predgbi)*100,2))\nprint(\"Classification Report:\",classification_report(y_test,predgbi))","execution_count":94,"outputs":[]},{"metadata":{"_uuid":"c3ce676382c75eccbcfe605f0a0dd04a48053abc"},"cell_type":"markdown","source":"In the above GBC code, I have commented the parameter evaluation code because it takes a lot of time for execution. In version 9 of this notebook , I ran only the parameter evaluation code, I got the parameters of: <br>\n    * Learning Rate = 0.1\n    * Max Depth = 5\n    * Max Features = 0.5 \nHence, I used those features directly from Version 10 onwards for faster execution. If you want to see the running, you can either run version 9 or uncomment that part.\n    "},{"metadata":{"_uuid":"5e9eb8e5158760e1bf3802b3367eb1186817a804"},"cell_type":"markdown","source":"(6). K - Nearest Neighbor Classifier"},{"metadata":{"trusted":true,"_uuid":"4121c390f2f1e4e099bf219b0f01be11f0a85d3d","collapsed":true},"cell_type":"code","source":"# K Nearest Neighbour Algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(x_train,y_train)\npredknn = knn.predict(x_test)\nprint(\"Confusion Matrix for K Neighbors Classifier:\")\nprint(confusion_matrix(y_test,predknn))\nprint(\"Score: \",round(accuracy_score(y_test,predknn)*100,2))\nprint(\"Classification Report:\")\nprint(classification_report(y_test,predknn))","execution_count":95,"outputs":[]},{"metadata":{"_uuid":"f56bf490b885f9303b3d231a4531e00e49cd6b2a"},"cell_type":"markdown","source":"(7). XGBoost Classifier"},{"metadata":{"trusted":true,"_uuid":"b79d5587fed38dc6fed549de0f6e1c2744f78cb2","collapsed":true},"cell_type":"code","source":"# XGBoost Classifier\nimport xgboost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(x_train,y_train)\npredxgb = xgb.predict(x_test)\nprint(\"Confusion Matrix for XGBoost Classifier:\")\nprint(confusion_matrix(y_test,predxgb))\nprint(\"Score: \",round(accuracy_score(y_test,predxgb)*100,2))\nprint(\"Classification Report:\")\nprint(classification_report(y_test,predxgb))","execution_count":96,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"485d33ad9d0dded433a4fc7cf05924256467db2f","collapsed":true},"cell_type":"code","source":"# MULTILAYER PERCEPTRON CLASSIFIER\nfrom sklearn.neural_network import MLPClassifier\nmlp = MLPClassifier()\nmlp.fit(x_train,y_train)\npredmlp = mlp.predict(x_test)\nprint(\"Confusion Matrix for Multilayer Perceptron Classifier:\")\nprint(confusion_matrix(y_test,predmlp))\nprint(\"Score:\",round(accuracy_score(y_test,predmlp)*100,2))\nprint(\"Classification Report:\")\nprint(classification_report(y_test,predmlp))","execution_count":97,"outputs":[]},{"metadata":{"_uuid":"4fdf36796200d49cb226e0afc62d0e33906f3fd6"},"cell_type":"markdown","source":"From the above algorithm modelling, we can see that: \n    *  Multilayer Perceptron = 77.57%\n    * Multinomial Naive Bayes = 76.94%\n    * Gradient Boosting Classifier = 73.87%\n    * XGBoost Classifier = 70.81%\n    * Random Forest Classifier = 67.57%\n    * Decision Tree = 65.5%\n    * K Neighbor Classifier = 61.35%\n    * Support Vector Machine  = 59.1%\n"},{"metadata":{"_uuid":"4d1750493932ab569709498cbaa290fc6f4a0a07"},"cell_type":"markdown","source":"Since multilayer perceptron classifier has the best score, let us use it to predict a random positive review, a random average review and a random negative review!"},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"7a0d19cac82fa69142ab0e6fbd9c1ac05c001d48"},"cell_type":"code","source":"# POSITIVE REVIEW\npr = data['text'][0]\nprint(pr)\nprint(\"Actual Rating: \",data['stars'][0])\npr_t = vocab.transform([pr])\nprint(\"Predicted Rating:\")\nmlp.predict(pr_t)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bfb82ff1085c6af91635c34819c6287b25848a42","collapsed":true},"cell_type":"code","source":"# AVERAGE REVIEW\nar = data['text'][16]\nprint(ar)\nprint(\"Actual Rating: \",data['stars'][16])\nar_t = vocab.transform([ar])\nprint(\"Predicted Rating:\")\nmlp.predict(ar_t)[0]","execution_count":84,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true,"_uuid":"feb2cfec74130318701c0d0197a632d9d266609c"},"cell_type":"code","source":"# NEGATIVE REVIEW\nnr = data['text'][16]\nprint(nr)\nprint(\"Actual Rating: \",data['stars'][23])\nnr_t = vocab.transform([nr])\nprint(\"Predicted Rating:\")\nmlp.predict(nr_t)[0]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6c74cde9c07113026bc8c033c6f36c3a71c6b42b"},"cell_type":"code","source":"count = data['stars'].value_counts()\nprint(count)","execution_count":16,"outputs":[]},{"metadata":{"_uuid":"26e1de99725f6763637d0290892afaa5233da179"},"cell_type":"markdown","source":"From the above, we can see that predictions are biased towards positive reviews. We can see that the dataset has more positive reviews as compared to negative reviews. <br>\nI think I can fix it by normalizing the dataset to have equal number of reviews - thereby removing the bias. "}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}